# -*- coding: utf-8 -*-
"""Feature selection on best selling products.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JFhxs4dpsWqzWX69tOn3hw7WSIYWwG5J
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
df = pd.read_parquet(r'/content/drive/MyDrive/Norvartis Datathon/train_data.parquet')

# Add "Unknown" category to 'main_channel' and "U" to 'ther_area'
df['main_channel'] = df['main_channel'].cat.add_categories("Unknown")
df['ther_area'] = df['ther_area'].cat.add_categories("U")

# Fill missing values with "Unknown" for 'main_channel' and "U" for 'ther_area'
df['main_channel'].fillna("Unknown", inplace=True)
df['ther_area'].fillna("U", inplace=True)

# Fill missing values with 0 for 'hospital_rate'
df['hospital_rate'].fillna(0, inplace=True)

# Create a new column 'year' extracting the year from the 'date' column
df['year'] = df['date'].dt.year

# Create lag columns for 'wd_perc' (assuming this is the column you want to lag)
df['lag_1'] = df['wd_perc'].shift(1).fillna(0)
df['lag_2'] = df['wd_perc'].shift(2).fillna(0)
df['lag_3'] = df['wd_perc'].shift(3).fillna(0)

# Display concise information about the DataFrame, including data types and non-null counts
df.info()

# Create a pivot table
pivot_table = pd.pivot_table(df, values='monthly', index=['country', 'brand'], aggfunc=np.mean)
pivot_table.reset_index(inplace=True)

# print(pivot_table.info())
pivot = pivot_table.sort_values('monthly', ascending=False)

# Initialize an empty list to store the top rows for each country
top_rows = []

# Loop through unique countries in the pivot df
for country in pivot["country"].unique():
    # Select the top row for the current country
    top_row = pivot[pivot["country"] == country].iloc[[0], :]

    # Append the top row to the list
    top_rows.append(top_row)

# Concatenate the list of top rows into a new DataFrame
top_brands_in_country = pd.concat(top_rows)

# Display the resulting DataFrame
print(top_brands_in_country)

# Create an array with brands and countries
brands_and_countries_array = top_brands_in_country[['brand', 'country']].values

# Create different DataFrames based on brand and country combinations
filtered_dfs = {}
for brand, country in brands_and_countries_array:
    filtered_df_name = f"df_{brand}_{country}"
    filtered_df = df[(df['brand'] == brand) & (df['country'] == country)].copy()
    filtered_dfs[filtered_df_name] = filtered_df
    filtered_df.drop(["brand", "country", "monthly", "ther_area", "hospital_rate", "main_channel"], axis=1, inplace=True)

# Check some of the resulting DataFrames
from itertools import islice
for name, filtered_df in islice(filtered_dfs.items(), 5):
    print(f"\nDataFrame {name}:")
    display(filtered_df.head(1))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import pandas as pd

# Convertir los valores del diccionario en una lista
dfs_list = list(filtered_dfs.values())

# Initialize an empty DataFrame to store feature importances
importance_aggregate_df = pd.DataFrame(columns=['Feature'])

# Iterate through all created dataframes and extract the feature importance they have on a random forest model
for i, top_df in enumerate(dfs_list):
    # Select features and target for each model
    X = top_df.drop('phase', axis=1)
    y = top_df['phase']

    # Define numerical and categorical features
    numerical_features = X.select_dtypes(include=['float64', 'int64']).columns
    categorical_features = X.select_dtypes(include=['category']).columns

    # Combine transformers into a preprocessor using ColumnTransformer
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_features),
            ('cat', OneHotEncoder(), categorical_features)
        ])

    # Apply preprocessing to the entire feature set and convert to DataFrame
    X_preprocessed = pd.DataFrame(preprocessor.fit_transform(X), columns=numerical_features.tolist())

    # Split the preprocessed data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)

    # Create a Random Forest Regressor
    rf_model = RandomForestRegressor()

    # Create a pipeline with model training step only (preprocessing is already done)
    pipeline = Pipeline(steps=[('model', rf_model)])

    # Fit the pipeline to the training data
    pipeline.fit(X_train, y_train)

    # Extract feature importances
    feature_importances = pipeline.named_steps['model'].feature_importances_

    # Create a DataFrame with column names and feature importances
    importance_df = pd.DataFrame({'Feature': X_preprocessed.columns, 'Importance': feature_importances})

    # Merge the importance information to the aggregate DataFrame
    importance_aggregate_df = pd.merge(importance_aggregate_df, importance_df, on="Feature", how="outer", suffixes=(i,i+1))
# Print the aggregated feature importances across all iterations
print("\nAggregate Feature Importances:")
print(importance_aggregate_df)

import seaborn as sns
import matplotlib.pyplot as plt

# Transpose the DataFrame using the transpose() method
transposed_df = importance_aggregate_df.transpose()

# Set the features as column names
transposed_df.columns = transposed_df.iloc[0]
transposed_df = transposed_df[1:]

# Check the new df with all the importances per feature
display(transposed_df.head())

# Plot the importance distribution of each feature
plt.figure(figsize=(16, 8))
sns.boxplot( data=transposed_df)
plt.xticks(rotation=45, ha='right')
plt.title('Distribution of Feature Importances')
plt.show()